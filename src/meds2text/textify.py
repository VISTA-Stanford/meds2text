""" 
MEDS text linearization code.

- (1) TODO: Tabular data export (MEDS event -> properties)
- (2) LUMIA XML: Imposes heirarchical structure on data and includes metadata on provenance 
- (3) LUMIA JSON: JSON format for MEDS data 
- (4) FHIR-like: Fake FHIR-like format generated by an LLM 

FHIR-Like Transformation Prompt
Notes: This is just a spitball test of what a real FHIR feed might look like. It's not a real FHIR feed.
```
Read this XML schema documentation for FHIR Bundles
- https://hl7.org/fhir/xml.html
- https://hl7.org/fhir/bundle.html

I want a simple Python function that transforms the following XML example into a "FHIR-like" representation. Ideally, 
we would generate an exact, compliant transformation into FHIR. However, since this may be difficult, we want a 
representation that shares similarities in terms of tagging, nesting and verbosity of markup. The goal is to capture 
surface syntactic signal for training an LLM. 

Here is the example XML:
{XML}
```

This transformation script imposes a number of standard transformations for
OMOP data sourced from Stanford's STARR EHR data. 

Example Usage:

# MedAlign 
python src/meds2text/textify.py \
--path_to_meds /Users/jfries/Desktop/foobar/meds_reader_omop_medalign/ \
--path_to_ontology data/athena_omop_ontologies/ \
--path_to_metadata data/omop_metadata/ \
--path_to_output data/medalign_lumia_xml/ \
--exclude_props clarity_table \
--format lumia_xml \
--include_contexts person providers care_sites \
--apply_transforms \
--n_processes 1 


# ARPA-H Feb 2025 
python src/vista/meds_to_text_mp.py \
--path_to_meds data/meds/meds_vista_omop_confidential_feb2025_db/ \
--path_to_ontology data/dependencies/omop_ontology_v3 \
--path_to_metadata data/dependencies/metadata/ \
--path_to_output data/lumia_xml_oncology_feb2025/ \
--format lumia_xml \
--include_contexts person providers care_sites \
--exclude_props clarity_table \
--apply_transforms \
--batch_mode \
--batch_size 5000 \
--n_processes 64


python src/vista/meds_to_text_mp.py \
--path_to_meds data/meds/meds_vista_omop_confidential_feb2025_db/ \
--path_to_ontology data/dependencies/omop_ontology_v3 \
--path_to_metadata data/dependencies/metadata/ \
--path_to_output data/lumia_xml_oncology_feb2025/ \
--format lumia_xml \
--include_contexts person providers care_sites \
--exclude_props clarity_table \
--apply_transforms \
--n_processes 1 \
--test_mode

"""

import argparse
import collections
from datetime import datetime, timedelta
import json
import logging
import multiprocessing
import os
import re
import time
from typing import Any, Dict, Optional, Set, List
import xml.etree.ElementTree as ET

from dateutil.relativedelta import relativedelta
from lxml import etree
from lxml.etree import Element, SubElement, tostring
import pandas as pd

import meds_reader
from meds_reader.transform import MutableEvent, MutableSubject
from meds2text.ontology import OntologyDescriptionLookupTable
from meds2text.transforms import *


def parse_args():
    parser = argparse.ArgumentParser(description="Convert MEDS to text")
    parser.add_argument("--path_to_meds", type=str, help="")
    parser.add_argument("--path_to_output", type=str, help="")
    parser.add_argument(
        "--path_to_ontology", type=str, help="", default="omop_ontology"
    )
    parser.add_argument("--path_to_metadata", type=str, help="")
    parser.add_argument(
        "--format",
        type=str,
        help="",
        default="lumia_xml",
        choices=["lumia_xml", "lumia_json", "tabular", "fhir_like_json"],
    )
    parser.add_argument("--apply_transforms", action="store_true")
    parser.add_argument(
        "--include_contexts",
        nargs="+",
        choices=["person", "providers", "care_sites"],
        help="Specify metadata types to include (choose from: person, provider, care_site)",
    )
    parser.add_argument(
        "--exclude_props",
        nargs="+",
        choices=["clarity_table", "image_series_uid", "image_study_uid", "visit_id"],
        help="Specify properties to exclude from the output",
    )
    parser.add_argument(
        "--batch_mode",
        action="store_true",
        help="Enable batch mode (write patient timelines to batches)",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=2500,
        help="Batch size for output files in batch mode (default: 2500)",
    )
    parser.add_argument(
        "--test_mode", action="store_true", help="run a small test with a 10 subjects"
    )
    # New argument: number of processes for parallel processing.
    parser.add_argument(
        "--n_processes",
        type=int,
        default=1,
        help="Number of processes to use for multiprocessing (default: 1)",
    )
    args = parser.parse_args()
    return args


#
# Transformation Functions
#


def is_within_time_window(
    timestamp1: str, timestamp2: str, time_window_minutes: int = 1
) -> bool:
    """Check if two timestamps are within a given time window"""
    delta = abs(timestamp1 - timestamp2)
    return delta <= datetime.timedelta(minutes=time_window_minutes)


def create_mutable_event(event) -> MutableEvent:
    """Convert MEDS"""
    properties = {k: v for k, v in event if k not in ("time", "code")}
    return MutableEvent(event.time, event.code, properties)


def to_mutable_subject(subject):
    """Convert MEDS subject to a mutable object"""
    return MutableSubject(
        subject_id=subject.subject_id,
        events=[create_mutable_event(event) for event in subject.events],
    )


def omop_drop_orphan_events(
    subject: meds_reader.transform.MutableSubject,
) -> meds_reader.transform.MutableSubject:
    """Remove events that are missing their parent visit, identied by `visit_id.
    This can happen due to several reasons, e.g., when generating extracts with
    STARRForge due to datetime <18, >90 scrubbing of events, where a visit runs
    over a datetime boundary. In other words, the start dateime <90 and the end
    datetime of the visit is >90.
    """
    # only keep visits if they have a valid start and end datetime
    visit_ids = set()
    for event in subject.events:
        if event.table != "visit":
            continue
        if hasattr(event, "visit_id") and event.end is not None:
            visit_ids.add(event.visit_id)

    # filter out orphan events
    subject.events = [
        event
        for event in subject.events
        if event.table in {"person"} or getattr(event, "visit_id", None) in visit_ids
    ]
    return subject


#####################################
# flowsheet parsing


def get_flowsheet_dict(observation_json):
    """
    Converts Stanford observation JSON to canonical pipe-delimited format.
    Format: "display_name|value|unit"
    """
    components = {"display": None, "value": None, "unit": None}

    for item in observation_json.get("values", []):
        source = item.get("source", "")
        val = str(item["value"]) if item["value"] is not None else None

        if "meas_value" in source:
            components["value"] = val
        elif "units" in source:
            components["unit"] = val
        elif "disp_name" in source:
            components["display"] = val

    return components


def convert_shc_date(date_int: int) -> datetime:
    """
    Converts a Stanford Health Care date integer (days since 1900-01-01)
    into a datetime object.
    """
    base_date = datetime(1840, 12, 31)  # Epic's base date
    return base_date + timedelta(days=date_int)


def convert_shc_time(time_int: int) -> timedelta:
    """
    Converts a Stanford Health Care time integer (seconds since midnight)
    into a timedelta.
    """
    return datetime.timedelta(seconds=time_int)


def interpret_key_value(anchor_datetime: datetime, key: str, value: int | str) -> str:
    """convert"""
    if isinstance(value, int) or (isinstance(value, str) and value.isdigit()):
        value = int(value)
        if "time" in key.lower():
            ts = anchor_datetime.replace(
                hour=0, minute=0, second=0, microsecond=0
            ) + convert_shc_time(value)
            return ts.strftime("%Y-%m-%d %H:%M")

        # TODO: Fix this when MUMPS dates are fixed upstream
        elif "date" in key.lower():
            # TODO: There is a bug here with invalid integer date values
            # ts = convert_shc_date(value)
            return "NULL"  # ts.strftime("%Y-%m-%d %H:%M")

    # if the value is not an int, return it as a string
    return str(value) if value else "NULL"


def import_flowsheet_events(
    subject: meds_reader.transform.MutableSubject,
) -> meds_reader.transform.MutableSubject:
    """Flowsheet values are stored as a JSON object. We transform these
    to be properties of the event object, applying
    - date/time integers: some timestamps are represented in integer offset time.
                          we convert these to datetime strings like timestamp
    """
    events = []
    for event in subject.events:

        if event.code == "STANFORD_OBS/Flowsheet":
            flowsheet_data = json.loads(event.text_value)
            props = {k: v for k, v in event if k not in ("time", "code")}
            if flowsheet_data:
                fs_props = get_flowsheet_dict(flowsheet_data)
                # transform dates if necessary
                props["text_value"] = interpret_key_value(
                    event.time, fs_props["display"], fs_props["value"]
                )
                if fs_props["unit"] is not None:
                    props["unit"] = fs_props["unit"]
                props["name"] = fs_props["display"]

            events.append(MutableEvent(event.time, event.code, props))
        else:
            events.append(event)

    subject.events = events
    return subject


def remove_flowsheet_events(
    subject: meds_reader.transform.MutableSubject,
) -> meds_reader.transform.MutableSubject:
    """Filter out events based on table name"""
    # Define the set of tables to exclude
    excluded_codes = {"STANFORD_OBS/Flowsheet"}

    # Filter events based on the excluded tables
    subject.events = [
        event for event in subject.events if event.code not in excluded_codes
    ]
    return subject


#####################################


def omop_split_interval_events(
    subject: meds_reader.transform.MutableSubject,
) -> meds_reader.transform.MutableSubject:
    """Only tables visit, visit_detail, drug_exposure have start/end intervals"""
    events = []
    for event in subject.events:
        if (
            hasattr(event, "time")
            and hasattr(event, "end")
            and event.end
            and event.time
        ):
            # if the events start/end times are the same, we don't split
            # if event.time == event.end:
            if is_within_time_window(event.time, event.end):
                props = {k: v for k, v in event if k not in ("time", "code")}
                props["text_value"] = "start|end"
                events.append(MutableEvent(event.time, event.code, props))
                continue

            if event.end - event.time < timedelta(minutes=1):
                print(event.time, event.end)
            # split interval event into 2 events
            props = {k: v for k, v in event if k not in ("time", "code")}
            props["text_value"] = "start"
            events.append(MutableEvent(event.time, event.code, props))

            props = {k: v for k, v in event if k not in ("time", "code")}
            props["text_value"] = "end"
            events.append(MutableEvent(event.end, event.code, props))

        else:
            events.append(event)

    subject.events = sorted(events, key=lambda x: x.time)
    return subject


def apply_transforms(subject, *, transforms):
    for transform in transforms:
        subject = transform(subject)
    return subject


#
# Data Dependency Loaders
#


def read_metadata_file(path_to_metadata: str, base_name: str, **kwargs) -> pd.DataFrame:
    """
    Attempts to read a CSV file using either the .csv or .csv.tz extension.
    Raises a FileNotFoundError if neither file is found.
    """
    for ext in [".csv", ".csv.gz"]:
        file_path = os.path.join(path_to_metadata, f"{base_name}{ext}")
        if os.path.exists(file_path):
            return pd.read_csv(file_path, **kwargs)
    raise FileNotFoundError(
        f"Could not find file for {base_name} with extensions .csv or .csv.gz in {path_to_metadata}"
    )


def load_metadata(path_to_metadata: str) -> Dict[str, Any]:
    """
    This function loads several OMOP tables for materializing metadata
    for the MEDS text linearization. Assumes the following tables are present:

      - metadata.providers.csv or metadata.providers.csv.tz
      - metadata.payer_plan.csv or metadata.payer_plan.csv.tz
      - metadata.care_sites.csv or metadata.care_sites.csv.tz

    """

    def init_all_providers(metadata):
        """Requires provider and care_site tables"""
        n_errs = 0
        providers = {}
        for provider_id in metadata["provider"]:
            provider = provider_map.get(provider_id, None)
            props = {}
            props["provider_id"] = provider_id
            props["gender"] = (
                provider["gender_concept_id"] if provider["gender_concept_id"] else None
            )
            props["speciality"] = (
                provider["specialty_source_value"]
                if provider["specialty_source_value"]
                else None
            )
            props["year_of_birth"] = (
                int(provider["year_of_birth"]) if provider["year_of_birth"] else None
            )
            care_site_id = (
                provider["care_site_id"] if provider["care_site_id"] else None
            )
            props["care_site_id"] = care_site_id

            if care_site_id and care_site_id in care_site_map:
                props["care_site_name"] = metadata["care_site"][care_site_id]
            else:
                props["care_site_name"] = None

            providers[provider_id] = props

            if props["speciality"] is None:
                n_errs += 1

        metadata["provider"] = providers
        print(
            f"Missing Provider Speciality: {n_errs/len(metadata['provider'])*100:.1f}% {n_errs}/{len(metadata['provider'])}"
        )
        return metadata

    # providers
    provider_map_df = read_metadata_file(
        path_to_metadata, "metadata.providers", dtype=str
    )
    provider_map_df = provider_map_df.fillna("")
    provider_map_df["gender_concept_id"] = provider_map_df["gender_concept_id"].replace(
        {"8532": "FEMALE", "8507": "MALE", "0": ""}
    )
    provider_map = {
        row.provider_id: row._asdict() for row in provider_map_df.itertuples()
    }

    # payer plans
    payer_plan_df = read_metadata_file(
        path_to_metadata,
        "metadata.payer_plan",
        parse_dates=["payer_plan_period_start_DATE", "payer_plan_period_end_DATE"],
    )
    # Subset to only the relevant columns
    payer_plan_df = payer_plan_df[
        [
            "person_id",
            "payer_plan_period_start_DATE",
            "payer_plan_period_end_DATE",
            "payer_source_value",
        ]
    ]
    # Rename columns
    payer_plan_df.columns = [
        "person_id",
        "start_date",
        "end_date",
        "payer_source_value",
    ]

    payer_plan_map = collections.defaultdict(list)
    for row in payer_plan_df.itertuples():
        payer_plan_map[row.person_id].append(row._asdict())

    # care sites
    care_site_df = read_metadata_file(
        path_to_metadata, "metadata.care_sites", dtype=str
    )
    care_site_df["care_site_name"] = care_site_df["care_site_name"].fillna("NULL")
    care_site_map = {
        row.care_site_id: row.care_site_name for row in care_site_df.itertuples()
    }
    metadata = {
        "provider": provider_map,
        "payer_plan": payer_plan_map,
        "care_site": care_site_map,
    }
    metadata = init_all_providers(metadata)
    return metadata


def load_ontology(path_to_ontology: str) -> OntologyDescriptionLookupTable:
    ontology = OntologyDescriptionLookupTable()
    ontology.load(path_to_ontology)
    return ontology


#
# Helper Functions
#


def sanitize_xml_text(text: str) -> str:
    """
    Remove characters that are illegal in XML:
    - Control characters except tab (\x09), newline (\x0a), and carriage return (\x0d).
    - Null bytes (\x00).
    """
    # This regex matches characters in the ranges:
    #   \x00-\x08, \x0B-\x0C, and \x0E-\x1F
    return re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", text)


def calculate_age(start_date: datetime, end_date: datetime):
    if start_date > end_date:
        raise ValueError("Start date must be before end date")

    total_days = (end_date - start_date).days
    rd = relativedelta(end_date, start_date)
    # rd.years correctly calculates the number of whole years elapsed.
    return {"age_in_years": rd.years, "age_in_days": total_days}


def bin_events(intervals, events, excluded_tables=None):
    """
    Assigns each event to exactly one time interval. Events that do not fit into an existing interval
    are grouped into pseudo intervals.

    :param intervals: List of sorted (start, end) datetime tuples.
    :param events: List of event objects with `time` and `table` attributes.
    :param excluded_tables: Set of tables to exclude.
    :return: Tuple containing:
        1. A dictionary mapping each interval (including pseudo intervals) to its corresponding events.
        2. A dictionary mapping each interval to whether it's a real or pseudo interval.
    """
    excluded_tables = excluded_tables or set()

    # Sort events by time
    events = sorted(
        [e for e in events if e.table not in excluded_tables], key=lambda e: e.time
    )

    result = collections.defaultdict(list)
    # interval_types = {}  # Dictionary to map each interval to real or pseudo
    interval_indices = [-1] * len(events)  # Initialize assignment list with -1
    interval_idx = 0  # Current interval index

    # Assign events to intervals
    for i, event in enumerate(events):
        while interval_idx < len(intervals) and event.time > intervals[interval_idx][1]:
            interval_idx += 1

        if (
            interval_idx < len(intervals)
            and intervals[interval_idx][0] <= event.time <= intervals[interval_idx][1]
        ):
            result[intervals[interval_idx]].append(event)
            interval_indices[i] = interval_idx
            # interval_types[intervals[interval_idx]] = "real"

    # Identify and group unassigned events into pseudo intervals
    pseudo_intervals = []
    current_pseudo = []
    for i, event in enumerate(events):
        if interval_indices[i] == -1:
            current_pseudo.append(event)
        else:
            if current_pseudo:
                pseudo_interval = (current_pseudo[0].time, current_pseudo[-1].time)
                pseudo_intervals.append(pseudo_interval)
                result[pseudo_interval] = current_pseudo
                # interval_types[pseudo_interval] = "pseudo"
                current_pseudo = []

    if current_pseudo:
        pseudo_interval = (current_pseudo[0].time, current_pseudo[-1].time)
        pseudo_intervals.append(pseudo_interval)
        result[pseudo_interval] = current_pseudo
        # interval_types[pseudo_interval] = "pseudo"

    return dict(result)  # , interval_types


def is_non_overlapping(intervals):

    # First, sort the intervals by their start time.
    sorted_intervals = sorted(intervals, key=lambda iv: iv[0])

    # Then, iterate through the sorted intervals comparing each one with the previous.
    for i in range(1, len(sorted_intervals)):
        previous_interval = sorted_intervals[i - 1]
        current_interval = sorted_intervals[i]

        # If the current interval starts before the previous interval ends, they overlap.
        # Note: using '<' allows intervals that touch (i.e. current.start == previous.end).
        if current_interval[0] < previous_interval[1]:
            return False

    return True


def fuzzy_partition(rows, max_mismatches=1, max_timedelta=datetime.timedelta(hours=24)):
    """
    Partition rows into groups using a dominant id with fuzzy glitch merging,
    subject to two extra constraints:
      1. Never split a contiguous block of rows sharing the same timestamp,
         even if their id is different.
      2. Do not split a group if the gap between the last event in the current
         group and the next event is less than or equal to max_timedelta.

    Args:
      rows: List of tuples in the form (timestamp, id). Timestamps should be datetime objects.
            (The list must be sorted by timestamp.)
      max_mismatches: Maximum number of rows with non-dominant id that may be merged into a group.
      max_timedelta: A timedelta threshold. If the time gap between the last event
                     of the current group and the next event is <= max_timedelta,
                     force-merge the next row (and its block) into the current group.

    Returns:
      List of groups (each a list of rows).
    """
    if not rows:
        return []

    groups = []
    n = len(rows)
    i = 0

    while i < n:
        # Start a new group with the current row.
        current_group = [rows[i]]
        dominant_id = rows[i][1]
        glitch_count = 0
        i += 1

        while i < n:
            # Examine the next row.
            last_ts = current_group[-1][0]
            next_ts = rows[i][0]

            # Constraint 1: Never split rows with the same timestamp
            if next_ts == last_ts:
                # Add the entire contiguous block that shares this timestamp.
                same_ts = next_ts
                while i < n and rows[i][0] == same_ts:
                    current_group.append(rows[i])
                    # Even if the id differs, count it as a glitch.
                    if rows[i][1] != dominant_id:
                        glitch_count += 1
                    i += 1
                continue  # Continue checking following rows.

            # Constraint 2: Do not split if time gap is within max_timedelta
            time_diff = next_ts - last_ts
            if time_diff <= max_timedelta:
                # Even though the timestamp is different, the gap is small;
                # force-add the next row.
                current_group.append(rows[i])
                if rows[i][1] != dominant_id:
                    glitch_count += 1
                i += 1
                continue

            # Normal (non-forced) processing
            # Now the next row's timestamp is far enough from the current group.
            # Gather a contiguous block of rows that share the same id as rows[i].
            candidate_id = rows[i][1]
            block_start = i
            while i < n and rows[i][1] == candidate_id:
                # (We already handled same-timestamp rows above.)
                i += 1
            block_length = i - block_start

            if candidate_id == dominant_id:
                # No glitch: simply add the block.
                current_group.extend(rows[block_start:i])
            else:
                # This block represents a “glitch.”
                if glitch_count + block_length <= max_mismatches:
                    current_group.extend(rows[block_start:i])
                    glitch_count += block_length
                else:
                    # Too many glitches; end the current group BEFORE this block.
                    # (Reset i back so that this block will start the next group.)
                    i = block_start
                    break  # End current group.

        groups.append(current_group)

    return groups


def bin_by_time(events):
    time_bins = collections.defaultdict(list)
    for event in events:
        time_bins[event.time].append(event)
    return time_bins


#
# Export Format Converters
#


def xml_to_json(xml_string):
    """LLM-generated function to convert LUMIA XML to JSON"""

    def infer_type(value_str):
        """Infer the type of a string value and convert it appropriately"""
        # Remove whitespace
        value_str = value_str.strip()

        # Try integer
        try:
            return int(value_str)
        except ValueError:
            pass

        # Try float
        try:
            return float(value_str)
        except ValueError:
            pass

        # Return as string if no other type matches
        return value_str

    def parse_element(element):
        result = {}

        # Handle attributes
        if element.attrib:
            # Convert attribute values with type inference
            for key, value in element.attrib.items():
                result[key] = infer_type(value)

        # Handle text content
        if element.text and element.text.strip():
            if len(result) == 0 and not len(element):
                return infer_type(element.text)
            else:
                result["value"] = infer_type(element.text)

        # Handle child elements
        children = list(element)
        if children:
            if all(child.tag == children[0].tag for child in children):
                # List of similar elements
                result[children[0].tag] = [parse_element(child) for child in children]
            else:
                # Dictionary of different elements
                for child in children:
                    if child.tag in result:
                        if not isinstance(result[child.tag], list):
                            result[child.tag] = [result[child.tag]]
                        result[child.tag].append(parse_element(child))
                    else:
                        result[child.tag] = parse_element(child)

        return result

    try:
        root = ET.fromstring(xml_string)
        return parse_element(root)
    except ET.ParseError as e:
        return {"error": f"Invalid XML: {str(e)}"}


def xml_to_fhir_like(xml_string):
    # Parse XML
    root = ET.fromstring(xml_string)

    # Create FHIR-like Bundle structure
    bundle = {"resourceType": "Bundle", "type": "collection", "entry": []}

    # Process each encounter
    for encounter in root.findall(".//encounter"):
        # Create Encounter resource
        encounter_resource = {
            "resourceType": "Encounter",
            "status": "finished",
            "subject": {"reference": f"Patient/{root.get('person_id')}"},
        }
        bundle["entry"].append({"resource": encounter_resource})

        # Process patient information
        person = encounter.find(".//person")
        if person is not None:
            patient_resource = {
                "resourceType": "Patient",
                "id": root.get("person_id"),
                "birthDate": person.findtext(".//birthdate"),
                "gender": person.findtext(".//demographics/gender").lower(),
                "extension": [
                    {
                        "url": "ethnicity",
                        "valueString": person.findtext(".//demographics/ethnicity"),
                    }
                ],
            }
            bundle["entry"].append({"resource": patient_resource})

        # Process events
        for entry in encounter.findall(".//entry"):
            timestamp = entry.get("timestamp")

            for event in entry.findall("event"):
                event_type = event.get("type")

                if event_type == "measurement":
                    observation = {
                        "resourceType": "Observation",
                        "status": "final",
                        "code": {
                            "coding": [
                                {
                                    "system": event.get("code").split("/")[0],
                                    "code": event.get("code").split("/")[1],
                                    "display": event.get("name"),
                                }
                            ]
                        },
                        "effectiveDateTime": timestamp,
                        "valueQuantity": {
                            "value": event.text,
                            "unit": event.get("unit", ""),
                        },
                    }
                    bundle["entry"].append({"resource": observation})

                elif event_type == "note":
                    document_reference = {
                        "resourceType": "DocumentReference",
                        "status": "current",
                        "type": {
                            "coding": [
                                {
                                    "system": event.get("code").split("/")[0],
                                    "code": event.get("code").split("/")[1],
                                    "display": event.get("name"),
                                }
                            ]
                        },
                        "content": [
                            {
                                "attachment": {
                                    "contentType": "text/plain",
                                    "data": event.text,
                                }
                            }
                        ],
                    }
                    bundle["entry"].append({"resource": document_reference})

                elif event_type == "condition":
                    condition = {
                        "resourceType": "Condition",
                        "code": {
                            "coding": [
                                {
                                    "system": event.get("code").split("/")[0],
                                    "code": event.get("code").split("/")[1],
                                    "display": event.get("name"),
                                }
                            ]
                        },
                        "onsetDateTime": timestamp,
                    }
                    bundle["entry"].append({"resource": condition})

    return bundle


#
# Attribute Builders
#


def get_payer_plan_coverage(subject_id, index_time, payer_plan_map):
    coverage = payer_plan_map.get(subject_id, [])
    for period in coverage:
        if period["start_date"] <= index_time <= period["end_date"]:
            return period["payer_source_value"]
    return None


def get_person_values(
    subject: Any,
    ontology: Any,
    excluded_props: Optional[Set[str]] = None,
    metadata_props: Dict[str, str] = None,
) -> Dict[str, Any]:
    """
    Extracts personal attributes from subject events.

    Args:
        subject: An object containing events related to a person.
        ontology: An object with a `get_description` method for looking up event descriptions.
        excluded_props: A set of properties to exclude (default: empty set).

    Returns:
        A dictionary with extracted person attributes.
    """
    excluded_props = excluded_props or set()
    metadata_props = metadata_props or {}
    person: Dict[str, Any] = {"person_id": subject.subject_id}
    person.update(metadata_props)

    for event in getattr(subject, "events", []):
        if getattr(event, "table", None) == "person":
            if event.code == "MEDS_BIRTH":
                person["birth"] = event.time
            else:
                match = re.match(r"^(Ethnicity|Gender|Race)/", event.code)
                if match:
                    tag = match.group(1).lower()
                    person[tag] = ontology.get_description(event.code)
                else:
                    logger.error(f"Unexpected event code: {event.code}")

    return person


#
# Export Format
#


def datetime_to_str(dt: datetime) -> str:
    return dt.strftime("%Y-%m-%d %H:%M")


def person_to_xml(person: Dict) -> Element:
    person_elem = Element("person")

    # Birthdate
    if "birth" in person:
        birthdate_elem = SubElement(person_elem, "birthdate")
        birthdate_elem.text = person["birth"].strftime("%Y-%m-%d")

    # Age
    if "age_in_days" in person or "age_in_years" in person:
        age_elem = SubElement(person_elem, "age")
        if "age_in_days" in person:
            days_elem = SubElement(age_elem, "days")
            days_elem.text = str(person["age_in_days"])
        if "age_in_years" in person:
            years_elem = SubElement(age_elem, "years")
            years_elem.text = str(person["age_in_years"])

    # Demographics
    if "ethnicity" in person or "gender" in person or "race" in person:
        demographics_elem = SubElement(person_elem, "demographics")
        if "ethnicity" in person:
            ethnicity_elem = SubElement(demographics_elem, "ethnicity")
            ethnicity_elem.text = person["ethnicity"]
        if "gender" in person:
            gender_elem = SubElement(demographics_elem, "gender")
            gender_elem.text = person["gender"]
        if "race" in person:
            gender_elem = SubElement(demographics_elem, "race")
            gender_elem.text = person["race"]

    # Payer plan
    if "payer_plan" in person:
        payerplan_elem = SubElement(person_elem, "payerplan")
        payerplan_elem.text = person["payer_plan"]

    return person_elem


def event_to_xml(event, ontology, excluded_props: Set[str] = None) -> str:
    """ """
    excluded_props = excluded_props or set()
    remap = {"table": "type"}

    # code description / name
    if event.code != "STANFORD_OBS/Flowsheet":
        name = ontology.get_description(event.code) or ""

    attributes = {
        remap[key] if key in remap else key: value
        for key, value in event
        if key not in excluded_props
    }
    value = attributes.get("numeric_value") or attributes.get("text_value") or None

    # cleanup keynames
    if "name" not in attributes:
        attributes["name"] = name
    attributes.pop("numeric_value", None)
    attributes.pop("text_value", None)

    event_element = Element("event", **attributes)

    # event type (numeric, text, indicator)
    event_element.text = (
        None
        if value is None
        else (
            str(int(value))
            if isinstance(value, float) and value.is_integer()
            else (
                f"{float(value):.2f}"
                if isinstance(value, (float, int))
                else sanitize_xml_text(str(value))
            )
        )
    )

    # some text values are mappings to concepts, e.g., OMOP_CONCEPT_ID/{concept_id}
    if value is not None:
        code_value_description = ontology.get_description(str(value))
        if code_value_description:
            event_element.text = code_value_description

    return event_element


def entry_to_xml(entry: Dict[str, Any]) -> str:
    entry_elem = Element("entry", timestamp=datetime_to_str(entry["timestamp"]))

    for event in entry["events"]:
        entry_elem.append(event)

    return entry_elem


def dict_to_xml(
    data: List[Dict[str, Any]],
    parent_tag: str,
    child_tag: str,
    excluded_props: Set[str] = None,
) -> str:
    excluded_props = excluded_props or set()
    parent_elem = Element(parent_tag)

    for item in data:
        child_elem = SubElement(parent_elem, child_tag)
        for key, value in item.items():
            if key in excluded_props:
                continue
            if isinstance(value, dict):
                sub_elem = SubElement(child_elem, key)
                sub_elem.append(value)
            else:
                child_elem.set(key, str(value) if value is not None else "NULL")

    return parent_elem


#######################
# HACK Functions


def fix_image_events(xml_root):
    # Iterate through all event elements in the XML tree
    for event in xml_root.xpath(".//event"):
        # Check if the event has type="image"
        if event.get("type") == "image":
            # Get the attribute values
            anatomic_site = event.get("anatomic_site_source_value", "")
            modality = event.get("modality_source_value", "")
            # Create and set the new name attribute
            event.set("name", f"{anatomic_site} {modality}".strip())
    return xml_root


#######################


def process_subjects_chunk(subject_ids, args, process_id):
    """
    Process a chunk of subject IDs in a separate process.
    Each process loads its own copy of the ontology, metadata, and database.
    In batch mode, the process writes out its own TSV file with a process-specific prefix.
    """
    # Load shared resources
    ontology = load_ontology(args.path_to_ontology)
    metadata = load_metadata(args.path_to_metadata)
    database = meds_reader.SubjectDatabase(args.path_to_meds)

    # Transformation stack
    transforms = [
        to_mutable_subject,
        delta_encode,
        omop_drop_orphan_events,
        import_flowsheet_events,
        move_billing_codes,
        move_to_day_end,
        move_visit_start_to_first_event_start,
        remove_nones,
        switch_to_icd10cm,
        omop_split_interval_events,
        move_pre_birth,
    ]

    # some properties and tables we always exclude
    # additional exclusion props are configurable via the command line
    excluded_tables = {"person"}
    excluded_props = {
        "time",
        "end",
        "subject_id",
    }
    if args.exclude_props:
        excluded_props.update(args.exclude_props)

    if "providers" not in args.include_contexts:
        excluded_props.add("provider_id")
    if "care_sites" not in args.include_contexts:
        excluded_props.add("care_site_id")
        excluded_props.add("care_site_name")

    # Set conversion function and file extension based on output format
    if args.format == "lumia_json":
        convert_func = lambda root: json.dumps(xml_to_json(tostring(root)), indent=2)
        file_extension = "json"
    elif args.format == "lumia_xml":
        convert_func = lambda root: tostring(
            root, encoding="unicode", pretty_print=True
        )
        file_extension = "xml"
    elif args.format == "tabular":
        raise NotImplementedError("Tabular format not implemented")
    elif args.format == "fhir_like_json":
        convert_func = lambda root: json.dumps(
            xml_to_fhir_like(tostring(root)), indent=2
        )
        file_extension = "fhir-like.json"
    else:
        raise ValueError(f"Invalid format: {args.format}")

    batch_mode = getattr(args, "batch_mode", False)

    # Prepare batch output file stream
    if batch_mode:
        batch_filename = os.path.join(args.path_to_output, f"batch_{process_id}.jsonl")
        batch_file = open(
            batch_filename, "a", buffering=1, encoding="utf-8"
        )  # Line-buffered
    else:
        batch_file = None

    try:
        for i, subject_id in enumerate(subject_ids):
            subject = database[subject_id]
            if args.apply_transforms:
                subject = apply_transforms(subject, transforms=transforms)

            timestamps = [
                (event.time, event.visit_id)
                for event in subject.events
                if event.table not in excluded_tables
            ]

            person_values = get_person_values(subject, ontology, excluded_props)
            groups = fuzzy_partition(
                timestamps, max_mismatches=1, max_timedelta=timedelta(hours=24)
            )
            intervals = [
                (min(times), max(times))
                for times, _ in (zip(*group) for group in groups)
            ]

            if not is_non_overlapping(intervals):
                print("Warning, intervals overlap")

            intervals = bin_events(
                intervals, subject.events, excluded_tables=excluded_tables
            )

            data = {"encounters": []}
            for (start, end), events in intervals.items():
                encounter = {"events": [], "metadata": []}

                if args.include_contexts:
                    care_sites, providers = set(), set()
                    for event in events:
                        provider_id = getattr(event, "provider_id", None)
                        care_site_id = getattr(event, "care_site_id", None)
                        if provider_id:
                            providers.add(provider_id)
                        if care_site_id:
                            care_sites.add(care_site_id)

                    payer_plan = (
                        get_payer_plan_coverage(
                            subject_id, start, metadata["payer_plan"]
                        )
                        or ""
                    )
                    age = calculate_age(person_values["birth"], start)
                    person_values.update(
                        {
                            "payer_plan": payer_plan,
                            "age_in_years": age["age_in_years"],
                            "age_in_days": age["age_in_days"],
                        }
                    )
                    person = person_to_xml(person_values)

                    care_sites.update(
                        metadata["provider"][p]["care_site_id"]
                        for p in providers
                        if metadata["provider"][p]["care_site_id"]
                    )

                    providers_xml = dict_to_xml(
                        [metadata["provider"][p] for p in providers],
                        "providers",
                        "provider",
                        excluded_props,
                    )
                    care_sites_xml = dict_to_xml(
                        [
                            {
                                "care_site_id": cs,
                                "care_site_name": metadata["care_site"][cs],
                            }
                            for cs in care_sites
                        ],
                        "caresites",
                        "caresite",
                    )
                    if "person" in args.include_contexts:
                        encounter["metadata"].append(person)
                    if "care_sites" in args.include_contexts:
                        encounter["metadata"].append(care_sites_xml)
                    if "providers" in args.include_contexts:
                        encounter["metadata"].append(providers_xml)

                time_bins = bin_by_time(events)
                for ts, entries in time_bins.items():
                    entry = {
                        "timestamp": ts,
                        "events": [
                            event_to_xml(e, ontology, excluded_props) for e in entries
                        ],
                    }
                    encounter["events"].append(entry_to_xml(entry))

                enc_elem = Element("encounter")
                for elem in encounter["metadata"]:
                    enc_elem.append(elem)
                events_elem = SubElement(enc_elem, "events")
                for entry in encounter["events"]:
                    events_elem.append(entry)
                data["encounters"].append(enc_elem)

            root = Element("eventstream", person_id=str(subject_id))
            for enc in data["encounters"]:
                root.append(enc)

            # HACK apply fixes to XML elements
            root = fix_image_events(root)

            # Convert the root element to the desired format
            output = convert_func(root)

            if batch_mode:
                record = json.dumps({"subject_id": subject_id, "output": output})
                batch_file.write(record + "\n")
            else:
                out_filename = os.path.join(
                    args.path_to_output, f"{subject_id}.{file_extension}"
                )
                with open(out_filename, "w", encoding="utf-8") as f:
                    f.write(output)

            if args.test_mode and i > 20:
                break
    finally:
        if batch_file:
            batch_file.close()


def main(args):
    # HACK to fix os.fork() issue with polars (pre 3.14.0)
    multiprocessing.set_start_method("spawn", force=True)

    # Ensure output directory exists.
    if not os.path.exists(args.path_to_output):
        os.makedirs(args.path_to_output)

    if args.n_processes > 1:
        # Load database once to obtain the list of subject IDs.
        database = meds_reader.SubjectDatabase(args.path_to_meds)
        subject_ids = list(database)
        # Partition subject_ids into roughly equal chunks.
        n = len(subject_ids)
        chunk_size = (n // args.n_processes) + (n % args.n_processes > 0)
        chunks = [subject_ids[i : i + chunk_size] for i in range(0, n, chunk_size)]
        processes = []
        for idx, chunk in enumerate(chunks):
            p = multiprocessing.Process(
                target=process_subjects_chunk, args=(chunk, args, idx)
            )
            p.start()
            processes.append(p)
        for p in processes:
            p.join()
    else:
        # Single-process mode: process all subject IDs in one go.
        database = meds_reader.SubjectDatabase(args.path_to_meds)
        subject_ids = list(database)
        process_subjects_chunk(subject_ids, args, process_id=0)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logger = logging.getLogger(__name__)
    args = parse_args()

    start_time = time.time()
    main(args)
    elapsed_time = time.time() - start_time

    logger.info(f"Elapsed time: {elapsed_time:.2f} seconds")
